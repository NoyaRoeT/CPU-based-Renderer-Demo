1) Multiple duplicate vertex indices:
- Reason: A mistake during obj parsing, where I used x[0] instead of v0[0]. (x is a string and v0 is the actual indices)
- Effect: A large number of vertices were simply duplicates and were drawn on top of each other resulting in only a few points appearing.

2) Directly accounting for FOV (Without changing view plane dims) and Aspect Ratio
- We treat our view plane to have y-interval [-1, 1] and x-interval [-a, a].
- Hence, to scale x to NDC space with an interval of [-1, 1] we divide by a. Conversion from screen space is the same as x-interval is now [-1, 1].
- Considering we want the interval to be [-1, 1] and [-a, a], we do not change view plane dimensions. Instead, we scale the projected points in relation to the FOV.
- Rather than change view plane dimensions, we change distance to viewplane instead. Let distance to view plane be d. Then, d = 1 / tan(fov/2).
Using this formula we calculate an appropiate distance for our desired FOV.
- Notice that perspective projection is derived using similar triangles, where x/d = px/pz. Then: x = dpx/-pz.
- Overall: x = dpx /-apz. The -ve sign is either to make pz +ve or maybe because the -ve sign is not included in d.

3) Choosing the correct edge equation to use (left or right)
- I initially believed that a counter-clockwise winding triangle would use an edge equation that is positive on the left side of the edge. (Of course it can be anything)
- However, there is a need to remember that the edge equations are being computed using vertices in viewport space.
- In the transformation to viewport space, we flip the y-axis such that the +ve y-direction is now downwards. This reflection causes a reverse in the winding order of our triangles.
- This means that our originally counter-clockwise winding triangles are now clockwise winding. As such I had to use the opposite of what I expected.

4) Edge Equations for Triangles that are rotated till they face backward give -ve values
- Rather than a bug, I believe this is simply the nature of edge equations. When our triangles are rotated such that they now face backwards, the edge equation will
give a negative value as the winding order of the triangles are now reversed.
- I wonder how modern rasterizers deal with this problem?

5) Perspective-Correct Interpolation of Z-coords
- Basically, linearly Interpolating 1/Z using barycentric coordinates calculated in Viewport space(Screen space).

6) Incremental Method For Computing Edge Equation
- Since the Edge Equation is a Linear Equation that changes by either  x -> x + 1 or y -> y + 1 each time, we can simply compute the effects of that +1 and add it for each pixel
rather than computing the whole edge equation for every pixel.
- Considering (by - ay)(cx - ax), a +1 to cx would result in an additional +(by - ay). For -(bx - ax)(cy - ay), a +1 to cy results in an additional -(bx - ax). The - prefix is due to the -ve sign in front of (bx - ax).
- After implementing this the average ms per frames halved.

7) Direction of Rotation
- Seems to be counter-clockwise for +ve angle rotation. I assume this to be the case due to right-handed coordinate system.
For left-handed coordinate systems, positive rotations are clockwise.

8) Faces Not Rendering Properly after Adding Transformations:
- The transforms involved non orthogonal matrices, hence requiring the normal to be transformed by an inverse transpose of the transformation matrix, which I did not do.

9) Early Backface Culling
- Backface Culling is done before calling the vertex shader. We compute the ViewDir by taking one of the vertices of the triangle in view space and negating it. Then, we use the
dot product between ViewDir and face normal to determine whether it is back facing.
- To accomplish the above, we need to generate face normals for all triangles in our mesh. This is done simply by using a cross product.
- Need to remember to transform all face normals and vertices to view space before calculating Viewdir and computing dot product.

10) Polygon Meshes
- I had assumed that all meshes would be triangle meshes, thus when I imported free 3d meshes from online I had problems with triangle rasterization as I only considered 3 vertex indices and dropped the rest.
- This lead to multiple gaps in the final render of the mesh. To fix this, I decided to add functionality to triangulate Polygon meshes with 4 vertices for each face. (Why not more general? Cos I'm lazy)
- The rule for triangulating polygon meshes is that a face with N vertices will produce N - 2 triangles.
- To connect vertices to form triangles, simply take v[0], v[n + 1] and v[n + 2] where n is the face index.
- It is important to pay attention to both the original orientation of the polygons and the desired resulting orientation of the triangle. From there, we can adjust
the above formula accordingly to produce triangles with desired winding orientation.

11) Perspective Correct Barycentric Coordinates:
- To obtain Perpsective Correct Barycentric Coordinates, we simply divide the baryentric coordinates by the respective w values of the vertices and multiply it by the perspective-correct interpolated depth value.

12) Multiple Problems when implementing textures mainly from the fact that I had no clear idea on what I was doing
- The first problem stemmed from using a unique ptr to store textures in our model class. We copy models around but unique ptr's copy constructor is deleted and cannot be copied.
- When creating the texture class, I lost track of what values the data in my texture class actually were. Were they floats? Unsigned chars? It's important to have a clear idea on exactly
what format your data has so you can access and deal with them accordingly.

13) Poorly organised and thought out classes:
- When trying to get the classes to fit together, I ended up creating random functions, moving private members to public, making members pointers. I didn't have a clear idea of how things
were suppose to work or go tgt. This resulted in several random pointers and non-pointers scattered all over and random functions that were made as brute force solutions.
- Due to this, there is a need to go back and edit all the classes to make them more coherent and less confusing.
- Some of the places to note would be the model class and shader class. How should the models store textures and access them? How should data be passed into the shader class and how should the shader store them?
- Also consider the copy-control members and default constructor. How does copying work? Does this class even need to be copied? How should default construction work? Will the default constructor be called in a different class.
- Generally, I think it'll be great to have classes with dummy constructors that can be 'constructed' later on.

14) Shared_ptrs vs Unique_ptrs
- I completely forgot that unique_ptrs cannot be copied. I guess we use unique ptrs for stuff that will only be owned by a single object?
- We use shared_ptrs for objects that we want to copy I guess? The consideration here is that we would only be copying the ptr and not the object stored on the heap.

15) Scaling texture values to [0, 1] or [-1, 1] range
- Need to rmb to use floating point numbers to scale. Otherwise, the result will be a truncated integer value of either 0 or 1.
- Need to rmb to scale them back when using. Either scale it when retrieving the value or scale it as needed in the shader.

16) Coordinate-System Handedness for imported 3D Models
- Different model formats define vertex data using coordinate system of different handedness. E.g. Wavefront Obj uses a right-handed coordinate system to define vertices.
- If we were to say use a left-handed coordinate system for our program, we would need to flip the x-coordinates of the vertices that uses a right-handed coordinate system.

17) Tangent Space Normal Mapping:
- Essentially, we can think of the texture map lying on the triangle's plane with its x and y axes aligned to UV directions assigned to the triangle vertices.
- Using this, we calculate the tangent and bitangent vectors with the equation pi - pj = (ui - uj)t + (vi - vj)b where t and b are the tangent and bitangent vectors.
- We can get the equation for both edges v0v2 and v0v1, giving us 2 linear equations to solve for 2 unknowns. Converting to a matrix form, we can easily identify the inverse
transformation required to obtain t and b.
- We calculate an average unit length tangent vector for ea vertex, allowing us to create a smooth tangent field on the surface of a mesh via linear interpolation.
- We could interpolate vertex tangents and normals per pixel and use the result to retrieve a normal vector from the tangent space normal map, but this is an expensive per-pixel operation.
- Instead, we could convert ViewDir and LightDir to tangent space of ea (per-vertex operation). Then, we linearly interpolate the values in the fragment shader and use them for lighting calculations
with the tangent space normal vectors.
- An issue I had was forgetting the space that I was working with. Important to be clear on which space you're working in and intending to work in. The original normal and tangent vectors
for the mesh form tangent->object space matrix.

18) Tangent Space Normal Mapping : Correcting Handedness
- Our normals are alr in the correct position, what's left is to correct the handedness of our bitangent and tangent vectors. We only really need to correct one of them to obtain the right handedness.
- If we consider that cross product inherently follows the right-hand rule, we can check to see if our vectors follow a right-handed coordinate system using a cross product and a dot product.
- We simply compute the cross product of our tangent and bitangent vectors and use a dot product on the result of that with our normal. If < 0, the normal is facing the wrong way and we negate the tangent vector to obtain a right-handed coord system.
- We choose the tangent vector as we're only storing the tangent vector and not the bitangent vector.
- I don't really get why the origin of the image can be both top-left or bottom-left because of this though.

///////////
Why I am Stupid and spent hours debugging stupid things:
- Vector[i] / Vector[i].w, did this without assigning the result to anything.
Resulted in scaling of objects for functions that do this and not do this to be very different.
- Color *= intensity
This kept changing the state of my Color variable that was passed into the DrawTriangle function. The continous multiplication by a fraction made it hit 0
real fast.
